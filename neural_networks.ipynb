{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                    [-200.2, -234.1],\n",
    "                    [5000.5, 150.1],\n",
    "                    [6000.6, -125.1],\n",
    "                    [9000.9, -673.1]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.0\n",
      "std: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print('mean:', round(features_standardized[:, 0].mean()))\n",
    "print('std:', features_standardized[:, 0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(10,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.4161 - acc: 0.8134 - val_loss: 0.3348 - val_acc: 0.8589\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 90us/step - loss: 0.3248 - acc: 0.8636 - val_loss: 0.3298 - val_acc: 0.8604\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 84us/step - loss: 0.3157 - acc: 0.8667 - val_loss: 0.3309 - val_acc: 0.8598\n"
     ]
    }
   ],
   "source": [
    "# training binary classifier\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "# np load error\n",
    "# (data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "#     num_words=number_of_features)\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(\n",
    "    number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore original numpy after load\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# training a multiclass classifier\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_features = 5000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "data = reuters.load_data(num_words=number_features)\n",
    "\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "y_train = to_categorical(target_vector_train)\n",
    "y_test = to_categorical(target_vector_test)\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=100,\n",
    "                        activation='relu',\n",
    "                        input_shape=(number_features,)))\n",
    "network.add(layers.Dense(units=100, activation='relu'))\n",
    "network.add(layers.Dense(units=46, activation='softmax'))\n",
    "\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     y_train,\n",
    "                     epochs=3,\n",
    "                     verbose=0,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcaecfb1c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a regressor\n",
    "# import numpy as np\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras import models, layers\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# features, target = make_regression(n_samples=10000,\n",
    "#                                   n_features=3,\n",
    "#                                   n_informative=3,\n",
    "#                                   n_targets = 1,\n",
    "#                                   noise=0.0,\n",
    "#                                   random_state=0)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     features, target, test_size=0.33, random_state=0)\n",
    "\n",
    "# network = models.Sequential()\n",
    "# network.add(layers.Dense(units=32, \n",
    "#                         activation='relu',\n",
    "#                         input_shape=(X_train.shape[1],)))\n",
    "# network.add(layers.Dense(units=32, activation='relu'))\n",
    "# network.add(layers.Dense(units=1))\n",
    "\n",
    "# network.compile(loss='mse',\n",
    "#                optimizer='rmsprop',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = network.fit(X_train,\n",
    "#                      y_train,\n",
    "#                      epochs=10,\n",
    "#                      verbose=1,\n",
    "#                      batch_size=100,\n",
    "#                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 13s 517us/step - loss: 0.3393 - acc: 0.8649 - val_loss: 0.2794 - val_acc: 0.8892\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 8s 307us/step - loss: 0.2067 - acc: 0.9219 - val_loss: 0.2973 - val_acc: 0.8823\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 6s 258us/step - loss: 0.1689 - acc: 0.9371 - val_loss: 0.3241 - val_acc: 0.8767\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_features = 10000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', \n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))\n",
    "\n",
    "predicted_target = network.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing training history\n",
    "\n",
    "# import numpy as np\n",
    "# from keras.datasets import imdb\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras import models, layers\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# num_features = 10000\n",
    "\n",
    "# (data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "#     num_words=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 4s 143us/step - loss: 0.6430 - acc: 0.8087 - val_loss: 0.4973 - val_acc: 0.8558\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.4718 - acc: 0.8543 - val_loss: 0.4614 - val_acc: 0.8516\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.4416 - acc: 0.8551 - val_loss: 0.4284 - val_acc: 0.8594\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with weight regularization\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers, regularizers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_features = 1000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16,\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with early stopping\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "num_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='vall_loss', patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                           monitor='val_loss',\n",
    "                           save_best_only=True)]\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=20,\n",
    "                     callbacks=callbacks,\n",
    "                     verbose=0,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 6s 247us/step - loss: 0.6219 - acc: 0.6498 - val_loss: 0.4686 - val_acc: 0.8212\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.4917 - acc: 0.7772 - val_loss: 0.3788 - val_acc: 0.8499\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.4490 - acc: 0.8045 - val_loss: 0.3555 - val_acc: 0.8548\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with dropout\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dropout(0.2, input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 107us/step - loss: 0.4357 - acc: 0.8033 - val_loss: 0.3421 - val_acc: 0.8559\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.3251 - acc: 0.8636 - val_loss: 0.3264 - val_acc: 0.8614\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 61us/step - loss: 0.3122 - acc: 0.8695 - val_loss: 0.3308 - val_acc: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# saving model training progress\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "checkpoint = [ModelCheckpoint(filepath='models.hdf5')]\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     callbacks=checkpoint,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating using k-fold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "num_features=100\n",
    "\n",
    "features, target = make_classification(n_samples=10000,\n",
    "                                      n_features=num_features,\n",
    "                                      n_informative=3,\n",
    "                                      n_rendundant=0,\n",
    "                                      n_classes=2,\n",
    "                                      weights=[0.5, 0.5],\n",
    "                                      random_state=0)\n",
    "\n",
    "def create_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation='relu', inpute_shape=(\n",
    "        num_features,)))\n",
    "    network.add(layers.Dense(units=16, activation='relu'))\n",
    "    network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    network.compile(loss='binary_crossentropy',\n",
    "                   optimizers='rmsprop',\n",
    "                   metrics=['accuracy'])\n",
    "    \n",
    "    return network\n",
    "\n",
    "neural_net = KerasClassifier(build_fn=create_network,\n",
    "                            epochs=10,\n",
    "                            batch_size=100,\n",
    "                            verbose=1)\n",
    "\n",
    "cross_val_score(neural_net, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got multiple values for keyword argument 'allow_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c3e63613fb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mheight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/datasets/mnist.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://s3.amazonaws.com/img-datasets/mnist.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-804713a5f3ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnp_load_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_load_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-804713a5f3ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*a, **k)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mnp_load_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_load_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <lambda>() got multiple values for keyword argument 'allow_pickle'"
     ]
    }
   ],
   "source": [
    "# classifying images\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# np_load_old = np.load\n",
    "\n",
    "# np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "np.load = np_load_old\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "channels = 1\n",
    "height = 28\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "X_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "X_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = np_utils.to_categorical(target_train)\n",
    "y_test = np_utils.to_categorical(target_test)\n",
    "num_classes = X_test.shape[1]\n",
    "\n",
    "network = Sequential()\n",
    "network.add(Conv2D(filters=64,\n",
    "                  kernel_size=(5,5),\n",
    "                  input_shape=(channels, width, height),\n",
    "                  activation='relu'))\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation='relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "network.compile(X_train,\n",
    "               y_train,\n",
    "               epochs=2,\n",
    "               verbose=1,\n",
    "               batch_size=1000,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billie-kernel",
   "language": "python",
   "name": "billie-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
