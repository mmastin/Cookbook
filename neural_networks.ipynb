{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                    [-200.2, -234.1],\n",
    "                    [5000.5, 150.1],\n",
    "                    [6000.6, -125.1],\n",
    "                    [9000.9, -673.1]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.0\n",
      "std: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print('mean:', round(features_standardized[:, 0].mean()))\n",
    "print('std:', features_standardized[:, 0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(10,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load(path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 129us/step - loss: 0.4161 - acc: 0.8134 - val_loss: 0.3348 - val_acc: 0.8589\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 90us/step - loss: 0.3248 - acc: 0.8636 - val_loss: 0.3298 - val_acc: 0.8604\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 84us/step - loss: 0.3157 - acc: 0.8667 - val_loss: 0.3309 - val_acc: 0.8598\n"
     ]
    }
   ],
   "source": [
    "# training binary classifier\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_of_features = 1000\n",
    "\n",
    "# np load error\n",
    "# (data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "#     num_words=number_of_features)\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=number_of_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16, activation='relu', input_shape=(\n",
    "    number_of_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(features_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(features_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore original numpy after load\n",
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# training a multiclass classifier\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "number_features = 5000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "data = reuters.load_data(num_words=number_features)\n",
    "\n",
    "(data_train, target_vector_train), (data_test, target_vector_test) = data\n",
    "\n",
    "tokenizer = Tokenizer(num_words=number_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "y_train = to_categorical(target_vector_train)\n",
    "y_test = to_categorical(target_vector_test)\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=100,\n",
    "                        activation='relu',\n",
    "                        input_shape=(number_features,)))\n",
    "network.add(layers.Dense(units=100, activation='relu'))\n",
    "network.add(layers.Dense(units=46, activation='softmax'))\n",
    "\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     y_train,\n",
    "                     epochs=3,\n",
    "                     verbose=0,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcaecfb1c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training a regressor\n",
    "# import numpy as np\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras import models, layers\n",
    "# from sklearn.datasets import make_regression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# features, target = make_regression(n_samples=10000,\n",
    "#                                   n_features=3,\n",
    "#                                   n_informative=3,\n",
    "#                                   n_targets = 1,\n",
    "#                                   noise=0.0,\n",
    "#                                   random_state=0)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     features, target, test_size=0.33, random_state=0)\n",
    "\n",
    "# network = models.Sequential()\n",
    "# network.add(layers.Dense(units=32, \n",
    "#                         activation='relu',\n",
    "#                         input_shape=(X_train.shape[1],)))\n",
    "# network.add(layers.Dense(units=32, activation='relu'))\n",
    "# network.add(layers.Dense(units=1))\n",
    "\n",
    "# network.compile(loss='mse',\n",
    "#                optimizer='rmsprop',\n",
    "#                metrics=['mse'])\n",
    "\n",
    "# history = network.fit(X_train,\n",
    "#                      y_train,\n",
    "#                      epochs=10,\n",
    "#                      verbose=1,\n",
    "#                      batch_size=100,\n",
    "#                      validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 13s 517us/step - loss: 0.3393 - acc: 0.8649 - val_loss: 0.2794 - val_acc: 0.8892\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 8s 307us/step - loss: 0.2067 - acc: 0.9219 - val_loss: 0.2973 - val_acc: 0.8823\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 6s 258us/step - loss: 0.1689 - acc: 0.9371 - val_loss: 0.3241 - val_acc: 0.8767\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import models, layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_features = 10000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy', \n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))\n",
    "\n",
    "predicted_target = network.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.load = np_load_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing training history\n",
    "\n",
    "# import numpy as np\n",
    "# from keras.datasets import imdb\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras import models, layers\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# num_features = 10000\n",
    "\n",
    "# (data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "#     num_words=num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 4s 143us/step - loss: 0.6430 - acc: 0.8087 - val_loss: 0.4973 - val_acc: 0.8558\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 2s 74us/step - loss: 0.4718 - acc: 0.8543 - val_loss: 0.4614 - val_acc: 0.8516\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 71us/step - loss: 0.4416 - acc: 0.8551 - val_loss: 0.4284 - val_acc: 0.8594\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with weight regularization\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers, regularizers\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_features = 1000\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16,\n",
    "                        kernel_regularizer=regularizers.l2(0.01),\n",
    "                        activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: val_loss,val_acc,loss,acc\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with early stopping\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "num_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_features)\n",
    "X_train = tokenizer.sequences_to_matrix(data_train, mode='binary')\n",
    "X_test = tokenizer.sequences_to_matrix(data_test, mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='vall_loss', patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',\n",
    "                           monitor='val_loss',\n",
    "                           save_best_only=True)]\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=20,\n",
    "                     callbacks=callbacks,\n",
    "                     verbose=0,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 6s 247us/step - loss: 0.6219 - acc: 0.6498 - val_loss: 0.4686 - val_acc: 0.8212\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.4917 - acc: 0.7772 - val_loss: 0.3788 - val_acc: 0.8499\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.4490 - acc: 0.8045 - val_loss: 0.3555 - val_acc: 0.8548\n"
     ]
    }
   ],
   "source": [
    "# reducing overfitting with dropout\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dropout(0.2, input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 3s 107us/step - loss: 0.4357 - acc: 0.8033 - val_loss: 0.3421 - val_acc: 0.8559\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 1s 57us/step - loss: 0.3251 - acc: 0.8636 - val_loss: 0.3264 - val_acc: 0.8614\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 2s 61us/step - loss: 0.3122 - acc: 0.8695 - val_loss: 0.3308 - val_acc: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# saving model training progress\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,\n",
    "                        activation='relu',\n",
    "                        input_shape=(num_features,)))\n",
    "network.add(layers.Dense(units=16, activation='relu'))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "checkpoint = [ModelCheckpoint(filepath='models.hdf5')]\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     callbacks=checkpoint,\n",
    "                     verbose=1,\n",
    "                     batch_size=100,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating using k-fold\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "num_features=100\n",
    "\n",
    "features, target = make_classification(n_samples=10000,\n",
    "                                      n_features=num_features,\n",
    "                                      n_informative=3,\n",
    "                                      n_rendundant=0,\n",
    "                                      n_classes=2,\n",
    "                                      weights=[0.5, 0.5],\n",
    "                                      random_state=0)\n",
    "\n",
    "def create_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16, activation='relu', inpute_shape=(\n",
    "        num_features,)))\n",
    "    network.add(layers.Dense(units=16, activation='relu'))\n",
    "    network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    network.compile(loss='binary_crossentropy',\n",
    "                   optimizers='rmsprop',\n",
    "                   metrics=['accuracy'])\n",
    "    \n",
    "    return network\n",
    "\n",
    "neural_net = KerasClassifier(build_fn=create_network,\n",
    "                            epochs=10,\n",
    "                            batch_size=100,\n",
    "                            verbose=1)\n",
    "\n",
    "cross_val_score(neural_net, features, target, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mattmastin/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.5801 - acc: 0.8203 - val_loss: 0.1525 - val_acc: 0.9570\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.1868 - acc: 0.9447 - val_loss: 0.1012 - val_acc: 0.9686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd181d67dd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classifying images\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "# np_load_old = np.load\n",
    "\n",
    "# np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# np.load = np_load_old\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "\n",
    "X_train = data_train.reshape(data_train.shape[0], channels, height, width)\n",
    "\n",
    "X_test = data_test.reshape(data_test.shape[0], channels, height, width)\n",
    "\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_train = np_utils.to_categorical(target_train)\n",
    "y_test = np_utils.to_categorical(target_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "network = Sequential()\n",
    "network.add(Conv2D(filters=64,\n",
    "                  kernel_size=(5,5),\n",
    "                  input_shape=(channels, width, height),\n",
    "                  activation='relu'))\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Flatten())\n",
    "network.add(Dense(128, activation='relu'))\n",
    "network.add(Dropout(0.5))\n",
    "network.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "network.compile(loss='categorical_crossentropy',\n",
    "               optimizer='rmsprop',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "network.fit(X_train,\n",
    "               y_train,\n",
    "               epochs=2,\n",
    "               verbose=1,\n",
    "               batch_size=1000,\n",
    "               validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# improving performance with image augmentation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "augmentation = ImageDataGenerator(featurewise_center=True,\n",
    "                                 zoom_range=0.3,\n",
    "                                 width_shift_range=0.2,\n",
    "                                 horizontal_flip=True,\n",
    "                                 rotation_range=90)\n",
    "\n",
    "augment_images = augmentation.flow_from_directory('raw/images',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary',\n",
    "                                                 save_to_dir='processed/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit_generator(augment_images,\n",
    "                     steps_per_epoch=2000,\n",
    "                     epochs=5,\n",
    "                     validation_date=augment_images_test,\n",
    "                     validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 698s 28ms/step - loss: 0.6773 - acc: 0.5996 - val_loss: 0.6606 - val_acc: 0.5962\n",
      "Epoch 2/3\n",
      " 1000/25000 [>.............................] - ETA: 9:56 - loss: 0.6620 - acc: 0.5930"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c85a960bd54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                      validation_data=(X_test, target_test))\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/billie-env/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# classifying text\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models, layers\n",
    "\n",
    "np_load_old = np.load\n",
    "\n",
    "np.load = lambda *a, **k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "num_features = 1000\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(\n",
    "    num_words=num_features)\n",
    "\n",
    "X_train = sequence.pad_sequences(data_train, maxlen=400)\n",
    "X_test = sequence.pad_sequences(data_test, maxlen=400)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Embedding(input_dim=num_features, output_dim=128))\n",
    "network.add(layers.LSTM(units=128))\n",
    "network.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "history = network.fit(X_train,\n",
    "                     target_train,\n",
    "                     epochs=3,\n",
    "                     verbose=1,\n",
    "                     batch_size=1000,\n",
    "                     validation_data=(X_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "billie-kernel",
   "language": "python",
   "name": "billie-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
